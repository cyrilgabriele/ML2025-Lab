{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c35de42",
   "metadata": {},
   "source": [
    "# EuroSAT_MS – Pretrained Deep Learning Pipeline\n",
    "\n",
    "This notebook fine-tunes the Hugging Face `Rhodham96/EuroSatCNN` multispectral classifier on the 13-channel EuroSAT dataset and prepares a Kaggle submission using the resulting deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac33d11",
   "metadata": {},
   "source": [
    "## 1) Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca310b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "!pip install -q rasterio transformers huggingface_hub accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036fda54",
   "metadata": {},
   "source": [
    "## 2) Imports & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107185eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mrandom\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mmath\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtime\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mshutil\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mglob\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import os, random, math, time, shutil, glob, re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, get_cosine_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import rasterio\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44796f7b",
   "metadata": {},
   "source": [
    "## 3) Mount Drive and Extract EuroSAT_MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f699cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "ZIP_PATH = \"/content/drive/MyDrive/ML_HSG/EuroSAT_MS.zip\"\n",
    "DATA_ROOT = Path(\"/content/EuroSAT_MS\")\n",
    "\n",
    "if not DATA_ROOT.exists():\n",
    "    print(f\"Extracting dataset from {ZIP_PATH} ...\")\n",
    "    !unzip -q -o \"$ZIP_PATH\" -d \"/content\"\n",
    "else:\n",
    "    print(f\"Dataset already available at {DATA_ROOT}\")\n",
    "\n",
    "print(\"DATA_ROOT:\", DATA_ROOT.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641dda0f",
   "metadata": {},
   "source": [
    "## 4) Collect Samples and Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84111e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = []\n",
    "samples = []\n",
    "\n",
    "if DATA_ROOT.exists():\n",
    "    class_dirs = sorted([d for d in DATA_ROOT.iterdir() if d.is_dir()])\n",
    "    for d in class_dirs:\n",
    "        label = len(CLASS_NAMES)\n",
    "        CLASS_NAMES.append(d.name)\n",
    "        for tif_path in sorted(d.glob('*.tif')):\n",
    "            samples.append((tif_path, label))\n",
    "else:\n",
    "    raise FileNotFoundError(f\"{DATA_ROOT} not found. Check ZIP_PATH and rerun the extraction cell.\")\n",
    "\n",
    "CLASS_TO_IDX = {name: idx for idx, name in enumerate(CLASS_NAMES)}\n",
    "IDX_TO_CLASS = {idx: name for name, idx in CLASS_TO_IDX.items()}\n",
    "\n",
    "print(f\"Detected {len(CLASS_NAMES)} classes -> {CLASS_NAMES}\")\n",
    "print(f\"Total samples: {len(samples)}\")\n",
    "\n",
    "\n",
    "def pad_to_13_bands(arr: np.ndarray) -> np.ndarray:\n",
    "    arr = np.asarray(arr)\n",
    "    if arr.shape[0] == 13:\n",
    "        return arr\n",
    "    if arr.shape[0] == 12:\n",
    "        zero_band = np.zeros((1, *arr.shape[1:]), dtype=arr.dtype)\n",
    "        return np.concatenate([arr[:9], zero_band, arr[9:]], axis=0)\n",
    "    raise ValueError(f\"Expected 12 or 13 bands, got {arr.shape}\")\n",
    "\n",
    "\n",
    "def robust_normalize(arr: np.ndarray) -> np.ndarray:\n",
    "    arr = arr.astype(np.float32, copy=False)\n",
    "    out = np.empty_like(arr)\n",
    "    for i in range(arr.shape[0]):\n",
    "        band = arr[i]\n",
    "        lo, hi = np.percentile(band, [2, 98])\n",
    "        if hi > lo:\n",
    "            band = (band - lo) / (hi - lo)\n",
    "        else:\n",
    "            min_v, max_v = band.min(), band.max()\n",
    "            if max_v > min_v:\n",
    "                band = (band - min_v) / (max_v - min_v)\n",
    "            else:\n",
    "                band = np.zeros_like(band)\n",
    "        out[i] = np.clip(band, 0.0, 1.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_multispectral(path: Path) -> np.ndarray:\n",
    "    with rasterio.open(path) as src:\n",
    "        arr = src.read()  # (C,H,W)\n",
    "    arr = pad_to_13_bands(arr)\n",
    "    arr = robust_normalize(arr)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def show_sample(path: Path, title: str = None) -> None:\n",
    "    arr = load_multispectral(path)\n",
    "    rgb = np.stack([arr[3], arr[2], arr[1]], axis=-1)\n",
    "    ndvi = (arr[7] - arr[3]) / (arr[7] + arr[3] + 1e-6)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(np.clip(rgb, 0.0, 1.0))\n",
    "    plt.title(title or path.parent.name)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(ndvi, cmap='RdYlGn')\n",
    "    plt.title('NDVI')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if samples:\n",
    "    show_sample(samples[0][0], title=f\"Example: {samples[0][0].parent.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b4d20f",
   "metadata": {},
   "source": [
    "## 5) Train / Validation / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ac186",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = np.array([str(p) for p, _ in samples])\n",
    "labels = np.array([label for _, label in samples])\n",
    "indices = np.arange(len(samples))\n",
    "\n",
    "idx_trainval, idx_test = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.20,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "labels_trainval = labels[idx_trainval]\n",
    "idx_train, idx_val = train_test_split(\n",
    "    idx_trainval,\n",
    "    test_size=0.20,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=labels_trainval\n",
    ")\n",
    "\n",
    "train_samples = [samples[i] for i in idx_train]\n",
    "val_samples = [samples[i] for i in idx_val]\n",
    "test_samples = [samples[i] for i in idx_test]\n",
    "\n",
    "print(f\"Train: {len(train_samples)} | Val: {len(val_samples)} | Test: {len(test_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4858aa1f",
   "metadata": {},
   "source": [
    "## 6) Load Hugging Face EuroSatCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e69a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"Rhodham96/EuroSatCNN\"\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForImageClassification.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Align label mappings with our dataset ordering\n",
    "model.config.label2id = {cls: idx for idx, cls in enumerate(CLASS_NAMES)}\n",
    "model.config.id2label = {idx: cls for cls, idx in model.config.label2id.items()}\n",
    "\n",
    "# Inject dropout in the classification head for additional regularisation\n",
    "head_dropout = 0.4\n",
    "if hasattr(model, 'classifier'):\n",
    "    original_head = model.classifier\n",
    "    if isinstance(original_head, nn.Sequential):\n",
    "        model.classifier = nn.Sequential(nn.Dropout(head_dropout), *list(original_head.children()))\n",
    "    else:\n",
    "        model.classifier = nn.Sequential(nn.Dropout(head_dropout), original_head)\n",
    "elif hasattr(model, 'fc'):\n",
    "    model.fc = nn.Sequential(nn.Dropout(head_dropout), model.fc)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "param_total = sum(p.numel() for p in model.parameters())\n",
    "param_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Trainable parameters: {param_trainable/1e6:.2f}M / Total: {param_total/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ce121",
   "metadata": {},
   "source": [
    "## 7) Dataset & DataLoaders with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb00b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSATDataset(Dataset):\n",
    "    def __init__(self, samples, processor, train: bool = False, augment: bool = False):\n",
    "        self.samples = samples\n",
    "        self.processor = processor\n",
    "        self.train = train\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def random_augment(self, arr: np.ndarray) -> np.ndarray:\n",
    "        # arr is (H, W, C)\n",
    "        if random.random() < 0.5:\n",
    "            arr = np.flip(arr, axis=0).copy()\n",
    "        if random.random() < 0.5:\n",
    "            arr = np.flip(arr, axis=1).copy()\n",
    "        if random.random() < 0.5:\n",
    "            k = random.randint(1, 3)\n",
    "            arr = np.rot90(arr, k=k, axes=(0, 1)).copy()\n",
    "        if random.random() < 0.3:\n",
    "            noise = np.random.normal(0.0, 0.02, size=arr.shape).astype(np.float32)\n",
    "            arr = np.clip(arr + noise, 0.0, 1.0)\n",
    "        return arr\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        path, label = self.samples[idx]\n",
    "        arr = load_multispectral(path)  # (C, H, W)\n",
    "        arr = np.moveaxis(arr, 0, -1)   # (H, W, C)\n",
    "        if self.train and self.augment:\n",
    "            arr = self.random_augment(arr)\n",
    "        inputs = self.processor(images=arr, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0)\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = EuroSATDataset(train_samples, processor, train=True, augment=True)\n",
    "val_dataset = EuroSATDataset(val_samples, processor, train=False, augment=False)\n",
    "test_dataset = EuroSATDataset(test_samples, processor, train=False, augment=False)\n",
    "\n",
    "def make_loader(ds, shuffle=False):\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "train_loader = make_loader(train_dataset, shuffle=True)\n",
    "val_loader = make_loader(val_dataset)\n",
    "test_loader = make_loader(test_dataset)\n",
    "\n",
    "print(f\"Batches -> train: {len(train_loader)} | val: {len(val_loader)} | test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e53a1af",
   "metadata": {},
   "source": [
    "## 8) Optimiser, Regularisation & Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d4ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "LEARNING_RATE = 2e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LABEL_SMOOTHING = 0.05\n",
    "GRAD_CLIP = 1.0\n",
    "PATIENCE = 4\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = steps_per_epoch * EPOCHS\n",
    "warmup_steps = max(10, int(0.1 * total_steps))\n",
    "\n",
    "criterion_train = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "criterion_eval = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion=None):\n",
    "    model.eval()\n",
    "    loss_sum = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE, non_blocking=True)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=True)\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "            if criterion is not None:\n",
    "                loss = criterion(logits, labels)\n",
    "                loss_sum += loss.item() * labels.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_preds.append(preds.cpu())\n",
    "    avg_loss = (loss_sum / total) if (criterion is not None and total > 0) else float('nan')\n",
    "    acc = correct / total if total else 0.0\n",
    "    all_labels = torch.cat(all_labels).numpy() if all_labels else np.array([])\n",
    "    all_preds = torch.cat(all_preds).numpy() if all_preds else np.array([])\n",
    "    return {\"loss\": avg_loss, \"acc\": acc, \"labels\": all_labels, \"preds\": all_preds}\n",
    "\n",
    "\n",
    "history = []\n",
    "best_state = None\n",
    "best_val_acc = 0.0\n",
    "best_epoch = -1\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "    progress = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} [train]\", leave=False)\n",
    "    for batch in progress:\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE, non_blocking=True)\n",
    "        labels = batch[\"labels\"].to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            loss = criterion_train(outputs.logits, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        preds = outputs.logits.detach().argmax(dim=1)\n",
    "        epoch_loss += loss.item() * labels.size(0)\n",
    "        epoch_correct += (preds == labels).sum().item()\n",
    "        epoch_total += labels.size(0)\n",
    "\n",
    "        progress.set_postfix({\n",
    "            \"loss\": f\"{loss.item():.4f}\",\n",
    "            \"acc\": f\"{(preds == labels).float().mean().item():.3f}\"\n",
    "        })\n",
    "\n",
    "    train_loss = epoch_loss / epoch_total\n",
    "    train_acc = epoch_correct / epoch_total\n",
    "\n",
    "    val_metrics = evaluate(model, val_loader, criterion_eval)\n",
    "    history.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_loss\": val_metrics[\"loss\"],\n",
    "        \"val_acc\": val_metrics[\"acc\"]\n",
    "    })\n",
    "\n",
    "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f} train_acc={train_acc:.4f} | val_loss={val_metrics['loss']:.4f} val_acc={val_metrics['acc']:.4f}\")\n",
    "\n",
    "    if val_metrics[\"acc\"] > best_val_acc:\n",
    "        best_val_acc = val_metrics[\"acc\"]\n",
    "        best_epoch = epoch\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        patience_counter = 0\n",
    "        print(f\"--> New best model (val_acc={best_val_acc:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    torch.save(best_state, \"eurosatcnn_best.pt\")\n",
    "    print(f\"Restored best model from epoch {best_epoch} (val_acc={best_val_acc:.4f})\")\n",
    "else:\n",
    "    print(\"Warning: best_state is None – training may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d75d60",
   "metadata": {},
   "source": [
    "## 9) Evaluate on Held-out Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19608276",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = evaluate(model, test_loader, criterion_eval)\n",
    "print(f\"Test accuracy: {test_metrics['acc']:.4f}\")\n",
    "\n",
    "report = classification_report(test_metrics[\"labels\"], test_metrics[\"preds\"], target_names=CLASS_NAMES, digits=4)\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(test_metrics[\"labels\"], test_metrics[\"preds\"])\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_xticks(range(len(CLASS_NAMES)))\n",
    "ax.set_yticks(range(len(CLASS_NAMES)))\n",
    "ax.set_xticklabels(CLASS_NAMES, rotation=90)\n",
    "ax.set_yticklabels(CLASS_NAMES)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('Confusion Matrix (Test)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2dfb09",
   "metadata": {},
   "source": [
    "## 10) Kaggle Inference & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16760215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount (again) in case the session was reset before inference\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "KAGGLE_ZIP = \"/content/drive/MyDrive/ML_HSG/kaggle_data.zip\"\n",
    "KAGGLE_ROOT = Path(\"/content/kaggle_data\")\n",
    "KAGGLE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not any(KAGGLE_ROOT.iterdir()):\n",
    "    print(f\"Extracting {KAGGLE_ZIP} ...\")\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(KAGGLE_ZIP, 'r') as zf:\n",
    "        zf.extractall(KAGGLE_ROOT)\n",
    "else:\n",
    "    print(f\"Kaggle data already extracted at {KAGGLE_ROOT}\")\n",
    "\n",
    "search_patterns = [\n",
    "    str(KAGGLE_ROOT / \"**\" / \"testset\" / \"testset\" / \"*.npy\"),\n",
    "    str(KAGGLE_ROOT / \"**\" / \"testset\" / \"*.npy\"),\n",
    "    str(KAGGLE_ROOT / \"**\" / \"*.npy\"),\n",
    "]\n",
    "\n",
    "npy_paths = []\n",
    "for pattern in search_patterns:\n",
    "    hits = sorted(Path(p) for p in glob.glob(pattern, recursive=True))\n",
    "    if hits:\n",
    "        npy_paths = hits\n",
    "        break\n",
    "\n",
    "if len(npy_paths) == 0:\n",
    "    raise FileNotFoundError(\"No .npy files found in Kaggle archive\")\n",
    "\n",
    "print(f\"Found {len(npy_paths)} inference tiles. Example: {npy_paths[:3]}\")\n",
    "\n",
    "\n",
    "def extract_id(stem: str) -> int:\n",
    "    match = re.search(r\"(\\d+)$\", stem)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    digits = ''.join(ch for ch in stem if ch.isdigit())\n",
    "    return int(digits) if digits else -1\n",
    "\n",
    "\n",
    "class EuroSATNPYDataset(Dataset):\n",
    "    def __init__(self, paths, processor):\n",
    "        self.paths = paths\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        arr = np.load(path)\n",
    "        arr = pad_to_13_bands(arr)\n",
    "        arr = robust_normalize(arr)\n",
    "        arr = np.moveaxis(arr, 0, -1)\n",
    "        inputs = self.processor(images=arr, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0)\n",
    "        sample_id = extract_id(path.stem)\n",
    "        return {\"pixel_values\": pixel_values, \"id\": sample_id}\n",
    "\n",
    "\n",
    "kaggle_dataset = EuroSATNPYDataset(npy_paths, processor)\n",
    "kaggle_loader = DataLoader(kaggle_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "model.eval()\n",
    "pred_ids = []\n",
    "pred_indices = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(kaggle_loader, desc=\"Kaggle inference\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE, non_blocking=True)\n",
    "        logits = model(pixel_values=pixel_values).logits\n",
    "        preds = logits.argmax(dim=1).cpu().tolist()\n",
    "        pred_indices.extend(preds)\n",
    "        if isinstance(batch[\"id\"], torch.Tensor):\n",
    "            pred_ids.extend(batch[\"id\"].cpu().tolist())\n",
    "        else:\n",
    "            pred_ids.extend(batch[\"id\"])\n",
    "\n",
    "pred_labels = [IDX_TO_CLASS[idx] for idx in pred_indices]\n",
    "submission = pd.DataFrame({\"test_id\": pred_ids, \"label\": pred_labels})\n",
    "submission = submission.sort_values(\"test_id\").reset_index(drop=True)\n",
    "\n",
    "submission_path = Path(\"/content/submission.csv\")\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"Saved submission to {submission_path}\")\n",
    "print(submission.head())\n",
    "\n",
    "output_dir = Path(\"/content/drive/MyDrive/ML_HSG/kaggle_submissions\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "shutil.copy(submission_path, output_dir / submission_path.name)\n",
    "print(f\"Copied submission to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
