{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b1bf07",
   "metadata": {},
   "source": [
    "# Code Challenge 1 â€” B10 Reconstruction & Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a95877",
   "metadata": {},
   "source": [
    "This notebook trains a lightweight CNN to reconstruct the missing Sentinel-2 B10 band and then applies a pre-trained EuroSAT classifier to Kaggle tiles after inserting the predicted band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b68e34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyrilgabriele/anaconda3/envs/ml/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os, random, math, time, shutil, glob, re, zipfile, platform\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import rasterio\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and hasattr(torch, 'mps'):\n",
    "        torch.mps.manual_seed(seed)  # type: ignore[attr-defined]\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "def resolve_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = resolve_device()\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd83418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already available at /Users/cyrilgabriele/Documents/School/00_Courses/02_ML/03_project/EuroSAT_MS\n",
      "RUN_TIMESTAMP=20251110_150606\n",
      "Plots -> /Users/cyrilgabriele/Documents/School/00_Courses/02_ML/03_project/ML2025-Lab/cc_1/artifacts/plots\n",
      "Models -> /Users/cyrilgabriele/Documents/School/00_Courses/02_ML/03_project/ML2025-Lab/cc_1/artifacts/models\n",
      "Outputs -> /Users/cyrilgabriele/Documents/School/00_Courses/02_ML/03_project/ML2025-Lab/cc_1/outputs\n"
     ]
    }
   ],
   "source": [
    "def _env_path(var_name: str, default: Path) -> Path:\n",
    "    default_path = Path(default)\n",
    "    value = os.environ.get(var_name)\n",
    "    return Path(value).expanduser() if value else default_path\n",
    "\n",
    "REPO_ROOT = Path.cwd()\n",
    "DATA_BASE = _env_path('EUROSAT_DATA_BASE', Path('/Users/cyrilgabriele/Documents/School/00_Courses/02_ML/03_project'))\n",
    "DATA_ROOT = _env_path('EUROSAT_DATA_ROOT', DATA_BASE / 'EuroSAT_MS')\n",
    "DATA_ZIP = _env_path('EUROSAT_DATA_ZIP', DATA_BASE / 'EuroSAT_MS.zip')\n",
    "MODELS_DIR = _env_path('EUROSAT_MODELS_DIR', REPO_ROOT / 'artifacts' / 'models')\n",
    "PLOT_DIR = _env_path('EUROSAT_PLOTS_DIR', REPO_ROOT / 'artifacts' / 'plots')\n",
    "B10_MODEL_DIR = _env_path('EUROSAT_B10_MODEL_DIR', MODELS_DIR / 'cirrus_cnn')\n",
    "OUTPUT_DIR = _env_path('EUROSAT_OUTPUT_DIR', REPO_ROOT / 'outputs')\n",
    "KAGGLE_TEST_DIR = _env_path('EUROSAT_KAGGLE_TEST_DIR', DATA_BASE / 'kaggle_data' / 'testset' / 'testset')\n",
    "\n",
    "for path in (MODELS_DIR, PLOT_DIR, B10_MODEL_DIR, OUTPUT_DIR):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not DATA_ROOT.exists():\n",
    "    if DATA_ZIP.exists():\n",
    "        print(f\"Extracting dataset from {DATA_ZIP} ...\")\n",
    "        with zipfile.ZipFile(DATA_ZIP, 'r') as zip_ref:\n",
    "            zip_ref.extractall(DATA_ROOT.parent)\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Dataset directory {DATA_ROOT} not found. Update EUROSAT_DATA_ROOT or place an archive at {DATA_ZIP}.\"\n",
    "        )\n",
    "else:\n",
    "    print(f\"Dataset already available at {DATA_ROOT}\")\n",
    "\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"RUN_TIMESTAMP={RUN_TIMESTAMP}\")\n",
    "print(f\"Plots -> {PLOT_DIR}\")\n",
    "print(f\"Models -> {MODELS_DIR}\")\n",
    "print(f\"Outputs -> {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe40d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 10 classes\n",
      "Total samples: 27000\n"
     ]
    }
   ],
   "source": [
    "CLASS_NAMES = []\n",
    "samples = []\n",
    "\n",
    "if DATA_ROOT.exists():\n",
    "    class_dirs = sorted([d for d in DATA_ROOT.iterdir() if d.is_dir()])\n",
    "    for d in class_dirs:\n",
    "        label = len(CLASS_NAMES)\n",
    "        CLASS_NAMES.append(d.name)\n",
    "        for tif_path in sorted(d.glob('*.tif')):\n",
    "            samples.append((tif_path, label))\n",
    "else:\n",
    "    raise FileNotFoundError(f\"{DATA_ROOT} not found. Extract the archive first.\")\n",
    "\n",
    "CLASS_TO_IDX = {name: idx for idx, name in enumerate(CLASS_NAMES)}\n",
    "IDX_TO_CLASS = {idx: name for name, idx in CLASS_TO_IDX.items()}\n",
    "\n",
    "print(f\"Detected {len(CLASS_NAMES)} classes\")\n",
    "print(f\"Total samples: {len(samples)}\")\n",
    "\n",
    "DROP_BAND_INDEX = 10  # Sentinel-2 B10 (cirrus)\n",
    "KEEP_IDX_13 = np.array([i for i in range(13) if i != DROP_BAND_INDEX])\n",
    "KEEP_IDX_12 = np.array([i for i in range(12)])\n",
    "CIRRUS_SCALE = 10000.0\n",
    "CIRRUS_MODEL = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf8d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CirrusCNN(nn.Module):\n",
    "    def __init__(self, in_channels: int = 12, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, 1, kernel_size=1, bias=True)\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "def sample_patches(arr: np.ndarray, num_patches: int, patch_size: int, rng: np.random.Generator):\n",
    "    c, h, w = arr.shape\n",
    "    ps = min(patch_size, h, w)\n",
    "    patches_x = []\n",
    "    patches_y = []\n",
    "    for _ in range(num_patches):\n",
    "        top = int(rng.integers(0, max(1, h - ps + 1)))\n",
    "        left = int(rng.integers(0, max(1, w - ps + 1)))\n",
    "        patch = arr[:, top:top + ps, left:left + ps]\n",
    "        patches_x.append(patch[KEEP_IDX_13])\n",
    "        patches_y.append(patch[DROP_BAND_INDEX:DROP_BAND_INDEX + 1])\n",
    "    return patches_x, patches_y\n",
    "\n",
    "\n",
    "def _iter_patch_batches(tile_paths, patches_per_tile, patch_size, batch_size, rng: np.random.Generator, desc: str | None = None):\n",
    "    if not tile_paths:\n",
    "        return\n",
    "\n",
    "    order = np.arange(len(tile_paths))\n",
    "    rng.shuffle(order)\n",
    "    iterator = order\n",
    "    if desc is not None:\n",
    "        iterator = tqdm(order, desc=desc, leave=False)\n",
    "    batch_x, batch_y = [], []\n",
    "    for idx in iterator:\n",
    "        path = tile_paths[idx]\n",
    "        with rasterio.open(path) as src:\n",
    "            arr = src.read().astype(np.float32)\n",
    "        if arr.shape[0] != 13:\n",
    "            continue\n",
    "        arr = np.clip(arr, 0.0, CIRRUS_SCALE) / CIRRUS_SCALE\n",
    "        px, py = sample_patches(arr, patches_per_tile, patch_size, rng)\n",
    "        for x_patch, y_patch in zip(px, py):\n",
    "            batch_x.append(x_patch)\n",
    "            batch_y.append(y_patch)\n",
    "            if len(batch_x) == batch_size:\n",
    "                yield torch.from_numpy(np.stack(batch_x)).float(), torch.from_numpy(np.stack(batch_y)).float()\n",
    "                batch_x, batch_y = [], []\n",
    "    if batch_x:\n",
    "        yield torch.from_numpy(np.stack(batch_x)).float(), torch.from_numpy(np.stack(batch_y)).float()\n",
    "\n",
    "\n",
    "def fit_cirrus_cnn(sample_paths,\n",
    "                    patches_per_tile: int = 48,\n",
    "                    patch_size: int = 64,\n",
    "                    epochs: int = 10,\n",
    "                    batch_size: int = 128,\n",
    "                    lr: float = 5e-4,\n",
    "                    val_fraction: float = 0.1,\n",
    "                    max_tiles: int | None = None) -> dict:\n",
    "    if not sample_paths:\n",
    "        raise ValueError(\"No samples provided for cirrus CNN fitting\")\n",
    "\n",
    "    base_rng = np.random.default_rng(RANDOM_SEED)\n",
    "    paths = list(sample_paths)\n",
    "    if max_tiles is not None and len(paths) > max_tiles:\n",
    "        idx = base_rng.choice(len(paths), size=max_tiles, replace=False)\n",
    "        paths = [paths[i] for i in idx]\n",
    "    base_rng.shuffle(paths)\n",
    "\n",
    "    val_count = max(1, int(len(paths) * val_fraction))\n",
    "    val_paths = paths[:val_count]\n",
    "    train_paths = paths[val_count:]\n",
    "    if not train_paths:\n",
    "        raise ValueError(\"Not enough tiles for training after validation split\")\n",
    "\n",
    "    print(f\"[CirrusCNN] Training on {len(train_paths)} tiles (+{len(val_paths)} val) with {patches_per_tile} patches per tile per epoch.\")\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CirrusCNN().to(device)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    best_state = None\n",
    "    best_val = float('inf')\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_rng = np.random.default_rng(base_rng.integers(0, 1 << 32))\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_samples = 0\n",
    "        for xb, yb in _iter_patch_batches(train_paths, patches_per_tile, patch_size, batch_size, epoch_rng, desc=f'Train patches (epoch {epoch})'):\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "            train_samples += xb.size(0)\n",
    "        train_loss = train_loss / max(1, train_samples)\n",
    "\n",
    "        val_rng = np.random.default_rng(epoch_rng.integers(0, 1 << 32))\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in _iter_patch_batches(val_paths, patches_per_tile, patch_size, batch_size, val_rng, desc=f'Val patches (epoch {epoch})'):\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                preds = model(xb)\n",
    "                val_loss += criterion(preds, yb).item() * xb.size(0)\n",
    "                val_samples += xb.size(0)\n",
    "        val_loss = val_loss / max(1, val_samples)\n",
    "        print(f\"[CirrusCNN] epoch {epoch}/{epochs} train={train_loss:.6f} val={val_loss:.6f}\")\n",
    "\n",
    "        history.append({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss, 'train_samples': train_samples, 'val_samples': val_samples})\n",
    "\n",
    "        if val_loss < best_val and val_samples > 0:\n",
    "            best_val = val_loss\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    eval_rng = np.random.default_rng(RANDOM_SEED + 1234)\n",
    "    mae_sum = 0.0\n",
    "    mse_sum = 0.0\n",
    "    count = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in _iter_patch_batches(paths, patches_per_tile, patch_size, 256, eval_rng, desc='Evaluating cirrus CNN'):\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            preds = model(xb)\n",
    "            mae_sum += torch.nn.functional.l1_loss(preds, yb, reduction='sum').item()\n",
    "            mse_sum += torch.nn.functional.mse_loss(preds, yb, reduction='sum').item()\n",
    "            count += xb.size(0)\n",
    "    mae = mae_sum / max(1, count)\n",
    "    rmse = math.sqrt(mse_sum / max(1, count))\n",
    "\n",
    "    print(f\"[CirrusCNN] Aggregated over ~{count} patches | MAE={mae:.6f} | RMSE={rmse:.6f}\")\n",
    "\n",
    "    if history:\n",
    "        hist_df = pd.DataFrame(history)\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        ax.plot(hist_df['epoch'], hist_df['train_loss'], label='Train')\n",
    "        ax.plot(hist_df['epoch'], hist_df['val_loss'], label='Val')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('SmoothL1 Loss')\n",
    "        ax.set_title('CirrusCNN Training History')\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.3)\n",
    "        fig.tight_layout()\n",
    "        plot_path = PLOT_DIR / f'cirrus_cnn_history_{RUN_TIMESTAMP}.png'\n",
    "        fig.savefig(plot_path, dpi=180, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved CirrusCNN training plot to {plot_path}\")\n",
    "\n",
    "    model.cpu()\n",
    "    return {\n",
    "        'module': model,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'scale': CIRRUS_SCALE,\n",
    "    }\n",
    "\n",
    "\n",
    "def synthesize_cirrus(arr: np.ndarray) -> np.ndarray:\n",
    "    arr = np.asarray(arr, dtype=np.float32)\n",
    "    if CIRRUS_MODEL is None:\n",
    "        raise RuntimeError('CIRRUS_MODEL is None. Train or load the B10 reconstruction model before calling synthesize_cirrus.')\n",
    "\n",
    "    scale = CIRRUS_MODEL['scale']\n",
    "    arr_scaled = np.clip(arr, 0.0, scale) / scale\n",
    "    if arr.shape[0] == 13:\n",
    "        feats = arr_scaled[KEEP_IDX_13]\n",
    "    elif arr.shape[0] == 12:\n",
    "        feats = arr_scaled[KEEP_IDX_12]\n",
    "    else:\n",
    "        raise ValueError(f\"Expected 12 or 13 bands, got {arr.shape}\")\n",
    "\n",
    "    tensor = torch.from_numpy(feats).unsqueeze(0)\n",
    "    module = CIRRUS_MODEL['module']\n",
    "    module.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = module(tensor).squeeze(0).squeeze(0).numpy()\n",
    "    pred = np.clip(pred, 0.0, 1.0) * scale\n",
    "    return pred.astype(np.float32)\n",
    "\n",
    "\n",
    "def pad_to_13_bands(arr: np.ndarray) -> np.ndarray:\n",
    "    arr = np.asarray(arr, dtype=np.float32)\n",
    "    if arr.shape[0] == 13:\n",
    "        out = arr.copy()\n",
    "        out[DROP_BAND_INDEX] = synthesize_cirrus(arr)\n",
    "        return out\n",
    "    if arr.shape[0] == 12:\n",
    "        cirrus = synthesize_cirrus(arr)\n",
    "        return np.concatenate([arr[:DROP_BAND_INDEX], cirrus[np.newaxis, ...], arr[DROP_BAND_INDEX:]], axis=0)\n",
    "    raise ValueError(f\"Expected 12 or 13 bands, got {arr.shape}\")\n",
    "\n",
    "\n",
    "def robust_normalize(arr: np.ndarray) -> np.ndarray:\n",
    "    arr = arr.astype(np.float32, copy=False)\n",
    "    out = np.empty_like(arr)\n",
    "    for i in range(arr.shape[0]):\n",
    "        band = arr[i]\n",
    "        lo, hi = np.percentile(band, [2, 98])\n",
    "        if hi > lo:\n",
    "            band = (band - lo) / (hi - lo)\n",
    "        else:\n",
    "            min_v, max_v = band.min(), band.max()\n",
    "            if max_v > min_v:\n",
    "                band = (band - min_v) / (max_v - min_v)\n",
    "            else:\n",
    "                band = np.zeros_like(band)\n",
    "        out[i] = np.clip(band, 0.0, 1.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_multispectral(path: Path) -> np.ndarray:\n",
    "    with rasterio.open(path) as src:\n",
    "        arr = src.read()\n",
    "    arr = pad_to_13_bands(arr)\n",
    "    arr = robust_normalize(arr)\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0617de28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CirrusCNN] Training on 24300 tiles (+2700 val) with 64 patches per tile per epoch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m cirrus_paths = [p \u001b[38;5;28;01mfor\u001b[39;00m p, _ \u001b[38;5;129;01min\u001b[39;00m samples]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m CIRRUS_MODEL = \u001b[43mfit_cirrus_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcirrus_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatches_per_tile\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m CIRRUS_MODEL[\u001b[33m'\u001b[39m\u001b[33mmodule\u001b[39m\u001b[33m'\u001b[39m] = CIRRUS_MODEL[\u001b[33m'\u001b[39m\u001b[33mmodule\u001b[39m\u001b[33m'\u001b[39m].cpu()\n\u001b[32m      4\u001b[39m CIRRUS_MODEL[\u001b[33m'\u001b[39m\u001b[33mmodule\u001b[39m\u001b[33m'\u001b[39m].eval()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mfit_cirrus_cnn\u001b[39m\u001b[34m(sample_paths, patches_per_tile, patch_size, epochs, batch_size, lr, val_fraction, max_tiles)\u001b[39m\n\u001b[32m    107\u001b[39m yb = yb.to(device)\n\u001b[32m    108\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m preds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m loss = criterion(preds, yb)\n\u001b[32m    111\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mCirrusCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "cirrus_paths = [p for p, _ in samples]\n",
    "CIRRUS_MODEL = fit_cirrus_cnn(cirrus_paths, epochs=1, patches_per_tile=64, patch_size=64)\n",
    "CIRRUS_MODEL['module'] = CIRRUS_MODEL['module'].cpu()\n",
    "CIRRUS_MODEL['module'].eval()\n",
    "\n",
    "b10_bundle = {\n",
    "    'state_dict': CIRRUS_MODEL['module'].state_dict(),\n",
    "    'mae': CIRRUS_MODEL['mae'],\n",
    "    'rmse': CIRRUS_MODEL['rmse'],\n",
    "    'scale': CIRRUS_MODEL['scale'],\n",
    "    'drop_band_index': DROP_BAND_INDEX,\n",
    "    'keep_idx_12': KEEP_IDX_12,\n",
    "    'keep_idx_13': KEEP_IDX_13,\n",
    "}\n",
    "\n",
    "b10_model_path = B10_MODEL_DIR / f'cirrus_cnn_{RUN_TIMESTAMP}.pt'\n",
    "torch.save(b10_bundle, b10_model_path)\n",
    "print(f\"Saved B10 reconstruction weights to {b10_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'Rhodham96/EuroSatCNN'\n",
    "MODEL_SOURCE = 'auto'  # auto -> local first, fallback to HF\n",
    "LOCAL_MODEL_DIR_CANDIDATES = [\n",
    "    MODELS_DIR / 'Rhodham96-EuroSatCNN',\n",
    "    Path('./local_models/Rhodham96-EuroSatCNN'),\n",
    "    Path('./models/Rhodham96-EuroSatCNN'),\n",
    "    Path.home() / 'models' / 'Rhodham96-EuroSatCNN',\n",
    "]\n",
    "\n",
    "LOCAL_MODEL_DIR = next((p for p in LOCAL_MODEL_DIR_CANDIDATES if p.exists()), LOCAL_MODEL_DIR_CANDIDATES[0])\n",
    "LOCAL_MODEL_DEF = LOCAL_MODEL_DIR / 'model_def.py'\n",
    "LOCAL_MODEL_WEIGHTS = LOCAL_MODEL_DIR / 'pytorch_model.bin'\n",
    "\n",
    "print(f\"Loading classifier {MODEL_ID} (source={MODEL_SOURCE})\")\n",
    "print(f\"Resolved local directory: {LOCAL_MODEL_DIR}\")\n",
    "model = None\n",
    "processor = None\n",
    "image_size = 224\n",
    "\n",
    "local_available = LOCAL_MODEL_DEF.exists() and LOCAL_MODEL_WEIGHTS.exists()\n",
    "\n",
    "if MODEL_SOURCE in ('auto', 'local') and local_available:\n",
    "    import importlib.util\n",
    "    spec = importlib.util.spec_from_file_location('eurosat_local_model', LOCAL_MODEL_DEF)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    if not hasattr(module, 'EuroSATCNN'):\n",
    "        raise AttributeError(f\"EuroSATCNN class not found in {LOCAL_MODEL_DEF}\")\n",
    "    EuroSATCNN = module.EuroSATCNN\n",
    "    model = EuroSATCNN(num_classes=len(CLASS_NAMES))\n",
    "    state_dict = torch.load(LOCAL_MODEL_WEIGHTS, map_location='cpu')\n",
    "    if isinstance(state_dict, dict) and 'state_dict' in state_dict:\n",
    "        state_dict = state_dict['state_dict']\n",
    "    model.load_state_dict(state_dict)\n",
    "    image_size = getattr(model, 'image_size', getattr(model, 'input_resolution', 64))\n",
    "    if not hasattr(model, 'image_size'):\n",
    "        model.image_size = image_size\n",
    "    print(f\"Loaded local weights from {LOCAL_MODEL_WEIGHTS}\")\n",
    "\n",
    "if model is None and MODEL_SOURCE in ('auto', 'hf'):\n",
    "    print(f\"Falling back to Hugging Face hub for {MODEL_ID}\")\n",
    "    model = AutoModelForImageClassification.from_pretrained(MODEL_ID)\n",
    "    image_size = getattr(model.config, 'image_size', image_size)\n",
    "    try:\n",
    "        processor = AutoImageProcessor.from_pretrained(MODEL_ID)\n",
    "    except (OSError, IndexError) as hf_proc_err:\n",
    "        print(f\"Processor load failed: {hf_proc_err}\")\n",
    "        processor = None\n",
    "\n",
    "if processor is None:\n",
    "    print('Using fallback processor.')\n",
    "\n",
    "    class EuroSatFallbackImageProcessor:\n",
    "        def __init__(self, image_size: int = 224):\n",
    "            self.image_size = image_size\n",
    "\n",
    "        def _prep_single(self, image):\n",
    "            if isinstance(image, np.ndarray):\n",
    "                tensor = torch.from_numpy(image)\n",
    "                if tensor.dtype != torch.float32:\n",
    "                    tensor = tensor.float()\n",
    "            elif torch.is_tensor(image):\n",
    "                tensor = image.float()\n",
    "            else:\n",
    "                tensor = torch.from_numpy(np.asarray(image, dtype=np.float32))\n",
    "\n",
    "            if tensor.ndim != 3:\n",
    "                raise ValueError(f\"Expected image with 3 dims, got {tuple(tensor.shape)}\")\n",
    "\n",
    "            if tensor.shape[0] not in (3, 13):\n",
    "                tensor = tensor.permute(2, 0, 1)\n",
    "\n",
    "            tensor = tensor.clamp(0.0, 1.0)\n",
    "\n",
    "            if tensor.shape[1:] != (self.image_size, self.image_size):\n",
    "                tensor = torch.nn.functional.interpolate(\n",
    "                    tensor.unsqueeze(0),\n",
    "                    size=(self.image_size, self.image_size),\n",
    "                    mode='bilinear',\n",
    "                    align_corners=False\n",
    "                ).squeeze(0)\n",
    "\n",
    "            return tensor\n",
    "\n",
    "        def __call__(self, images, return_tensors=None):\n",
    "            if isinstance(images, (list, tuple)):\n",
    "                batch = torch.stack([self._prep_single(img) for img in images])\n",
    "            else:\n",
    "                batch = self._prep_single(images).unsqueeze(0)\n",
    "\n",
    "            if return_tensors in (None, 'pt'):\n",
    "                return {'pixel_values': batch}\n",
    "\n",
    "            raise ValueError(f\"Unsupported return_tensors value: {return_tensors!r}\")\n",
    "\n",
    "    processor = EuroSatFallbackImageProcessor(image_size=image_size)\n",
    "\n",
    "if not hasattr(model, 'config'):\n",
    "    from types import SimpleNamespace\n",
    "    model.config = SimpleNamespace()\n",
    "\n",
    "model.config.label2id = {cls: idx for idx, cls in enumerate(CLASS_NAMES)}\n",
    "model.config.id2label = {idx: cls for cls, idx in model.config.label2id.items()}\n",
    "model.config.num_labels = len(CLASS_NAMES)\n",
    "model.config.image_size = image_size\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print('Classifier ready for inference.')\n",
    "\n",
    "def get_model_logits(model, pixel_values):\n",
    "    try:\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "    except TypeError:\n",
    "        outputs = model(pixel_values)\n",
    "    if isinstance(outputs, torch.Tensor):\n",
    "        return outputs\n",
    "    if isinstance(outputs, dict):\n",
    "        if 'logits' in outputs:\n",
    "            return outputs['logits']\n",
    "        raise KeyError(\"Model output dict missing 'logits'\")\n",
    "    if hasattr(outputs, 'logits'):\n",
    "        return outputs.logits\n",
    "    raise RuntimeError('Model output does not contain logits')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183c4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSATNPYDataset(Dataset):\n",
    "    def __init__(self, paths, processor):\n",
    "        self.paths = paths\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        arr = np.load(path, allow_pickle=False)\n",
    "        if arr.ndim == 3 and arr.shape[0] not in (12, 13) and arr.shape[-1] in (12, 13):\n",
    "            arr = np.moveaxis(arr, -1, 0)\n",
    "        arr = pad_to_13_bands(arr)\n",
    "        arr = robust_normalize(arr)\n",
    "        arr = np.moveaxis(arr, 0, -1)\n",
    "        inputs = self.processor(images=arr, return_tensors='pt')\n",
    "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
    "        sample_id = self._extract_id(path.stem)\n",
    "        return {'pixel_values': pixel_values, 'id': sample_id}\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_id(stem: str) -> int:\n",
    "        match = re.search(r'(\\d+)$', stem)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        digits = ''.join(ch for ch in stem if ch.isdigit())\n",
    "        return int(digits) if digits else -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c6a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ROOT = KAGGLE_TEST_DIR\n",
    "if not TEST_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Expected test directory at {TEST_ROOT}\")\n",
    "\n",
    "npy_paths = sorted(TEST_ROOT.glob('*.npy'))\n",
    "if not npy_paths:\n",
    "    npy_paths = sorted(TEST_ROOT.glob('**/*.npy'))\n",
    "\n",
    "if len(npy_paths) == 0:\n",
    "    raise FileNotFoundError('No .npy files found in Kaggle test directory')\n",
    "\n",
    "print(f\"Found {len(npy_paths)} inference tiles\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0 if platform.system() == 'Darwin' else 2\n",
    "PIN_MEMORY = DEVICE.type == 'cuda'\n",
    "\n",
    "kaggle_dataset = EuroSATNPYDataset(npy_paths, processor)\n",
    "kaggle_loader = DataLoader(\n",
    "    kaggle_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "pred_ids = []\n",
    "pred_indices = []\n",
    "transfer_non_blocking = DEVICE.type == 'cuda'\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(kaggle_loader, desc='Kaggle inference'):\n",
    "        pixel_values = batch['pixel_values'].to(DEVICE, non_blocking=transfer_non_blocking)\n",
    "        logits = get_model_logits(model, pixel_values)\n",
    "        preds = logits.argmax(dim=1).cpu().tolist()\n",
    "        pred_indices.extend(preds)\n",
    "        if isinstance(batch['id'], torch.Tensor):\n",
    "            pred_ids.extend(batch['id'].cpu().tolist())\n",
    "        else:\n",
    "            pred_ids.extend(batch['id'])\n",
    "\n",
    "pred_labels = [IDX_TO_CLASS[idx] for idx in pred_indices]\n",
    "submission = pd.DataFrame({'test_id': pred_ids, 'label': pred_labels})\n",
    "submission = submission.sort_values('test_id').reset_index(drop=True)\n",
    "\n",
    "submission_name = f'submission_with_cirrus_{RUN_TIMESTAMP}.csv'\n",
    "submission_path = OUTPUT_DIR / submission_name\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"Saved submission to {submission_path}\")\n",
    "print(submission.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
