{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# EuroSAT_MS – Colab + Google Drive Version (Conventional ML Baseline)\n\nThis notebook is an updated solution that:\n- Mounts **Google Drive** and extracts your **EuroSAT_MS .zip** directly in Colab.\n- Covers the **visual tasks** (RGB view, NDVI, thresholded segmentation mask, per-band inspection).\n- For the **first modeling task**, uses **conventional machine learning** (feature engineering + scikit-learn classifiers) — **not** a CNN.\n- Compares simple models (Logistic Regression, SVM, RandomForest) on a validation split.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Mount Google Drive and extract the dataset\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 0,
      "outputs": [],
      "source": "from google.colab import drive\nfrom pathlib import Path\nimport os, glob, subprocess\n\ndrive.mount('/content/drive')\n\n# 🔧 Set this to your zip path in Drive if auto-discovery fails.\nZIP_PATH = None  # e.g. \"/content/drive/MyDrive/EuroSAT_MS.zip\"\n\nif ZIP_PATH is None:\n    # Try to auto-discover a EuroSAT-related zip in MyDrive\n    candidates = glob.glob(\"/content/drive/MyDrive/**/*.zip\", recursive=True)\n    euro = [p for p in candidates if \"eurosat\" in p.lower() or \"euro_sat\" in p.lower() or \"sentinel\" in p.lower()]\n    if euro:\n        ZIP_PATH = euro[0]\n        print(\"Auto-detected zip:\", ZIP_PATH)\n    else:\n        print(\"⚠️ Could not auto-detect. Please set ZIP_PATH to your zip in Drive.\")\n\nDEST = Path(\"/content/EuroSAT_MS\")\nDEST.mkdir(parents=True, exist_ok=True)\n\nif ZIP_PATH and Path(ZIP_PATH).exists():\n    print(\"Extracting... this may take a minute.\")\n    # use unzip (fast) and overwrite (-o) to be idempotent\n    !unzip -q -o \"$ZIP_PATH\" -d \"/content\"\n    # Often the zip contains a root folder; try to detect the proper data root\n    # We expect 10 class folders at the top-level of EuroSAT_MS path\n    # Try common names\n    import os\n    common_dirs = [\n        \"/content/EuroSAT_MS\", \n        \"/content/EuroSAT\", \n        \"/content/2750\", \n        \"/content/dataset\", \n        \"/content/EuroSATallBands\", \n        \"/content/EuroSAT_MS/EuroSAT_MS\"\n    ]\n    data_root = None\n    for d in common_dirs:\n        if Path(d).exists():\n            # Heuristic: contains many class folders with .tif files\n            subdirs = [p for p in Path(d).iterdir() if p.is_dir()]\n            if len(subdirs) >= 8:  # expect ~10\n                data_root = Path(d)\n                break\n    if data_root is None:\n        # fallback: find a directory under /content with >=8 subfolders\n        for p in Path(\"/content\").iterdir():\n            if p.is_dir():\n                subs = [q for q in p.iterdir() if q.is_dir()]\n                if len(subs) >= 8:\n                    data_root = p\n                    break\n    if data_root is None:\n        data_root = DEST\n    print(\"Using data root:\", data_root)\nelse:\n    data_root = DEST\n    print(\"⚠️ ZIP_PATH not set or file missing. Ensure data is available at:\", data_root)\n\nDATA_ROOT = Path(data_root)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Configuration & Utilities\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 0,
      "outputs": [],
      "source": "import os, sys, random, math, json, glob\nimport numpy as np\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\n\nprint(\"Data root:\", DATA_ROOT.resolve())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Load GeoTIFFs and Visualize (RGB, NDVI, Bands)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 0,
      "outputs": [],
      "source": "import numpy as np\nimport rasterio\nfrom pathlib import Path\n\n# Expected Sentinel-2 band order (EuroSAT-MS): indices 0..12 for 13 bands\n# NDVI: NIR=B8 (idx 7), RED=B4 (idx 3)\n\nCLASS_NAMES = []\nif DATA_ROOT.exists():\n    for d in sorted([p for p in DATA_ROOT.iterdir() if p.is_dir()]):\n        CLASS_NAMES.append(d.name)\nCLASS_TO_IDX = {c:i for i,c in enumerate(CLASS_NAMES)}\nprint(\"Detected classes:\", CLASS_NAMES)\n\ndef read_multiband_tif(path):\n    with rasterio.open(path) as src:\n        arr = src.read()  # (C,H,W)\n        arr = arr.astype(np.float32)\n        # robust per-band normalization to [0,1]\n        C,H,W = arr.shape\n        for i in range(C):\n            band = arr[i]\n            lo, hi = np.percentile(band, [2, 98])\n            if hi > lo:\n                band = (band - lo) / (hi - lo)\n            else:\n                bmin, bmax = band.min(), band.max()\n                band = (band - bmin) / (bmax - bmin + 1e-6)\n            arr[i] = np.clip(band, 0.0, 1.0)\n        return arr\n\n# Collect samples\nsamples = []\nfor cls in CLASS_NAMES:\n    for tif in (DATA_ROOT/cls).glob(\"*.tif\"):\n        samples.append((tif, CLASS_TO_IDX[cls]))\nprint(f\"Found {len(samples)} images.\")\n\n# Quick peek (1 sample)\nimport matplotlib.pyplot as plt\n\ndef ndvi_from_arr(arr):\n    nir = arr[7]  # B8\n    red = arr[3]  # B4\n    return (nir - red) / (nir + red + 1e-6)\n\ndef show_sample(path, title=\"Sample\"):\n    arr = read_multiband_tif(path)\n    rgb = np.stack([arr[3], arr[2], arr[1]], axis=-1)  # B4,B3,B2\n    ndvi = ndvi_from_arr(arr)\n\n    plt.figure(figsize=(8,4))\n    plt.subplot(1,2,1); plt.imshow(np.clip(rgb,0,1)); plt.title(f\"{title} – RGB\"); plt.axis(\"off\")\n    plt.subplot(1,2,2); plt.imshow(ndvi, cmap=\"RdYlGn\"); plt.title(\"NDVI\"); plt.axis(\"off\")\n    plt.show()\n\nif samples:\n    show_sample(samples[0][0], title=CLASS_NAMES[samples[0][1]])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### NDVI thresholding (vegetation mask)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 0,
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef threshold_ndvi(ndvi, thr=0.3):\n    return (ndvi > thr).astype(np.uint8)\n\nif samples:\n    arr = read_multiband_tif(samples[0][0])\n    ndvi = ndvi_from_arr(arr)\n    mask = threshold_ndvi(ndvi, thr=0.3)\n\n    plt.figure(figsize=(12,4))\n    plt.subplot(1,3,1); plt.imshow(np.stack([arr[3],arr[2],arr[1]], axis=-1)); plt.title(\"RGB\"); plt.axis(\"off\")\n    plt.subplot(1,3,2); plt.imshow(ndvi, cmap=\"RdYlGn\"); plt.title(\"NDVI\"); plt.axis(\"off\")\n    plt.subplot(1,3,3); plt.imshow(mask, interpolation=\"nearest\"); plt.title(\"NDVI > 0.3\"); plt.axis(\"off\")\n    plt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Per-band inspection\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 0,
      "outputs": [],
      "source": "def show_bands(arr):\n    import math\n    cols = 4\n    rows = math.ceil(arr.shape[0]/cols)\n    plt.figure(figsize=(12, 3*rows))\n    for i in range(arr.shape[0]):\n        plt.subplot(rows, cols, i+1)\n        plt.imshow(arr[i])\n        plt.title(f\"Band {i}\")\n        plt.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\nif samples:\n    arr = read_multiband_tif(samples[0][0])\n    show_bands(arr)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Feature Engineering for Conventional ML\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 0,
      "outputs": [],
      "source": "import numpy as np\n\ndef image_features(arr):\n    \"\"\"Return a 1D feature vector using simple, fast statistics per band + NDVI.\"\"\"\n    feats = []\n    # per-band stats\n    for i in range(arr.shape[0]):\n        band = arr[i]\n        p25, p50, p75 = np.percentile(band, [25,50,75])\n        feats.extend([band.mean(), band.std(), p25, p50, p75])\n    # NDVI stats\n    ndvi = ndvi_from_arr(arr)\n    p25, p50, p75 = np.percentile(ndvi, [25,50,75])\n    veg_ratio = (ndvi > 0.3).mean()\n    feats.extend([ndvi.mean(), ndvi.std(), p25, p50, p75, veg_ratio])\n    return np.array(feats, dtype=np.float32)\n\n# Optional: compact spatial features via downsampling then PCA later\ndef downsampled_flat(arr, target=16):\n    # simple spatial downsample by stride to roughly target x target per band\n    C,H,W = arr.shape\n    sx = max(1, H//target)\n    sy = max(1, W//target)\n    small = arr[:, ::sx, ::sy]\n    return small.reshape(-1)  # C * h * w\n\n# Build dataset matrices\nX_stats, y, paths = [], [], []\nX_flat = []\nfor p, lab in samples:\n    a = read_multiband_tif(p)\n    X_stats.append(image_features(a))\n    X_flat.append(downsampled_flat(a, target=16))\n    y.append(lab); paths.append(str(p))\n\nX_stats = np.stack(X_stats) if X_stats else np.empty((0,))\nX_flat  = np.stack(X_flat)  if X_flat  else np.empty((0,))\ny = np.array(y)\nprint(\"Feature shapes -> stats:\", X_stats.shape, \"| flat (for PCA):\", X_flat.shape, \"| labels:\", y.shape)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Train/Validation/Test Split\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 0,
      "outputs": [],
      "source": "from sklearn.model_selection import train_test_split\n\nXtr_stats, Xte_stats, ytr, yte, paths_tr, paths_te = train_test_split(\n    X_stats, y, paths, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n)\nXtr_stats, Xval_stats, ytr, yval, _, _ = train_test_split(\n    Xtr_stats, ytr, paths_tr, test_size=0.2, random_state=RANDOM_SEED, stratify=ytr\n)\n\nXtr_flat, Xte_flat, _, _ = train_test_split(\n    X_flat, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n)\nXtr_flat, Xval_flat, _, _ = train_test_split(\n    Xtr_flat, ytr, test_size=0.2, random_state=RANDOM_SEED, stratify=ytr\n)\n\nprint(\"Stats features -> train/val/test:\", Xtr_stats.shape, Xval_stats.shape, Xte_stats.shape)\nprint(\"Flat features  -> train/val/test:\", Xtr_flat.shape, Xval_flat.shape, Xte_flat.shape)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Conventional ML Baselines (scikit-learn)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 0,
      "outputs": [],
      "source": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Model A: Stats features -> Standardize -> Classifier\npipelines_stats = {\n    \"LogReg\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=2000, n_jobs=None, class_weight=\"balanced\"))]),\n    \"SVM-RBF\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(kernel=\"rbf\", C=10, gamma=\"scale\", class_weight=\"balanced\"))]),\n    \"RandomForest\": Pipeline([(\"clf\", RandomForestClassifier(n_estimators=300, max_depth=None, n_jobs=-1, class_weight=\"balanced_subsample\"))]),\n}\n\n# Model B: Flattened downsampled -> Standardize -> PCA -> Classifier\npipelines_flat = {\n    \"PCA+LogReg\": Pipeline([(\"scaler\", StandardScaler(with_mean=True)), (\"pca\", PCA(n_components=128, random_state=RANDOM_SEED)), (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))]),\n    \"PCA+SVM\": Pipeline([(\"scaler\", StandardScaler(with_mean=True)), (\"pca\", PCA(n_components=128, random_state=RANDOM_SEED)), (\"clf\", SVC(kernel=\"rbf\", C=10, gamma=\"scale\", class_weight=\"balanced\"))]),\n}\n\ndef eval_pipeline(name, pipe, Xtr, ytr, Xval, yval):\n    pipe.fit(Xtr, ytr)\n    pred = pipe.predict(Xval)\n    acc = accuracy_score(yval, pred)\n    print(f\"[{name}] val acc: {acc:.3f}\")\n    return acc, pipe\n\nbest = (0.0, None, None)\nprint(\"=== Using Stats Features ===\")\nfor name, pipe in pipelines_stats.items():\n    acc, fitted = eval_pipeline(name, pipe, Xtr_stats, ytr, Xval_stats, yval)\n    if acc > best[0]:\n        best = (acc, f\"stats::{name}\", fitted)\n\nprint(\"\\n=== Using Downsampled + PCA Features ===\")\nfor name, pipe in pipelines_flat.items():\n    acc, fitted = eval_pipeline(name, pipe, Xtr_flat, ytr, Xval_flat, yval)\n    if acc > best[0]:\n        best = (acc, f\"pca::{name}\", fitted)\n\nprint(\"\\nBest model on validation:\", best[1], \"with acc\", round(best[0],3))\nbest_model = best[2]\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Test Evaluation\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 0,
      "outputs": [],
      "source": "from sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Choose the right test features for the winning pipeline\nif best_model is None:\n    raise RuntimeError(\"No model trained. Check earlier cells.\")\n\nuse_stats = best[1].startswith(\"stats::\")\nXte = Xte_stats if use_stats else Xte_flat\n\nyhat = best_model.predict(Xte)\nprint(classification_report(yte, yhat, target_names=CLASS_NAMES))\n\ncm = confusion_matrix(yte, yhat)\nfig, ax = plt.subplots(figsize=(7,7))\nim = ax.imshow(cm, interpolation=\"nearest\")\nax.figure.colorbar(im, ax=ax)\nax.set(xticks=np.arange(len(CLASS_NAMES)), yticks=np.arange(len(CLASS_NAMES)),\n       xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n       ylabel=\"True label\", xlabel=\"Predicted label\")\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        ax.text(j, i, format(cm[i,j], \"d\"), ha=\"center\", va=\"center\")\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Notes on Distribution Shift (still relevant)\n\n- Ensure your splits avoid **tile leakage** (images from the same large scene should not be in both train and test).\n- Normalize per band using **training-set** statistics only.\n- Use augmentations that simulate realistic remote-sensing variability.\n- Consider domain adaptation or self-supervised pretraining if the target domain differs significantly.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9) (Optional) Deep Baseline\n\nIf later you also want a CNN baseline, you can add a small 13-channel CNN after the conventional ML section. For **Task 1**, the above **conventional ML** pipelines are sufficient.\n"
    }
  ]
}